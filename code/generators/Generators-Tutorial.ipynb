{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Generators in Python\n",
    "#### Charles M Rice\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data sources seem to keep getting bigger, leaving what humans can manage on our own in the dust. And while our computers have, thankfully, more than kept up, there are still datasets that bring the hardiest machine to its digital knees.\n",
    "\n",
    "Fortunately, Python has a built-in tool that lets us handle large sequences in batches: generator functions. \n",
    "\n",
    "Today, we're going to learn about these handy generators, see how they differ from 'normal' Python functions, and then build one to handle a real-world data-processing task.\n",
    "\n",
    "First things first: what is a generator?\n",
    "\n",
    "Officially:\n",
    ">***Generator***: \n",
    ">A function which returns a generator iterator. It looks like a normal function except that it contains yield expressions for producing a series of values usable in a for-loop or that can be retrieved one at a time with the next() function. [Python.org Glossary](https://docs.python.org/3/glossary.html#term-generator)\n",
    "\n",
    "The official definition is excellent, if you already know what a generator is. Let's try to translate into English.\n",
    "\n",
    "## Generators Iterate\n",
    "\n",
    "A generator is a function. So far, so good. We know what functions are: functions take an input, perform some operation, and return an output, whether it's a number, a string, or some other data structure. Then they end.\n",
    "\n",
    "For example, say you wanted a function that returned the first $n$ values of a Fibonacci sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below does exactly what it's supposed to do: calculate the numbers in the Fibonacci sequence recursively, store the whole thing in ```sequence```, and then display ```sequence``` all at once. It runs quickly and effectively, but once it's run, that's it. Unless you set ```fib(n)``` to a variable, ```sequence``` is gone. And if you want the next ten numbers in the sequence, you have to run it again, with a new value of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n == 0:\n",
    "        return [0]\n",
    "    elif n == 1:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        sequence = fib(n-1)\n",
    "        sequence.append(lst[-1] + lst[-2])\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion has its uses, certainly, but it also ties up more and more system resources as $n$ gets larger. And what if you didn't want all the integers at once, in a list, but several at a time? Therein lies the magic of generators!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our Fibonacci function now using a generator instead of recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fibonacci_generator():\n",
    "    a,b = 0,1 #This replaces the if/elif steps in the former function\n",
    "    while True: # The while statement here keeps the function 'live'\n",
    "        yield a #replaces the return statement\n",
    "        a,b = b, a+b\n",
    "\n",
    "fiboGen = fibonacci_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(fiboGen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that where we had a function with ```fibonacci```, we now have a generator with ```fibonacci_generator``` and ```fiboGen``` although the two functions do (more or less) the same thing. There are two big syntactical differences between the original function and our shiny new generator:\n",
    "- ```yield``` replaces ```return``` in a generator, and tells ```fibr``` to produce the output for one iteration, then wait for further instruction\n",
    "- ```next()``` is both the executor of the function and the further instruction. The function will use the most recent output of ```fibr``` to produce the next output, and will then wait until called again.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(fiboGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And again...\n",
    "next(fiboGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And again...\n",
    "next(fiboGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And again...\n",
    "next(fiboGen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, once it's executed, ```fiboGen``` will keep returning the next value in the sequence until the sun implodes or function is scrapped altogether, whichever comes first. So how can we use these nifty built-in generators to help us with data science? (Incidentally, very few languages have such an elegant method to handle this kind of work.)\n",
    "\n",
    "## So how often do you need the Fibonacci sequence?\n",
    "\n",
    "As every data scientist knows, sometimes you're working with a dataset that's too big to handle all at once, but too small to merit using Big Data tools. We're going to take a look at the [liquor sales in the state of Iowa](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy) for our example here. It's a large dataset of about 13 million rows by 24 columns; it would take a long time to load under the best of circumstances. Even this segment of it, which is about 10% of the whole, is a hefty 300+ MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "liquor = \"Iowa_Liquor_Sales_reduced.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunk(filename):\n",
    "    with open(filename, \"rb\") as table:\n",
    "        for line in table.readlines():\n",
    "            yield line\n",
    "            \n",
    "chunk = get_chunk(liquor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can even read the iterations into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pandas_chunk(filename):\n",
    "    for chunk in pd.read_csv(filename, chunksize=5):\n",
    "        yield chunk\n",
    "\n",
    "pc = pandas_chunk(liquor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila! Our data is now being parsed and loaded into a tidy pandas dataframe piece by piece. Since we are not loading the entire dataset at one time, we can see much more quickly how the data are structured, where we might need to do serious cleaning, or handling of null values, without tying up resources storing a 300 MB dataset in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Aside: Expressive Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other iterative elements of Python, generators can also be compressed into single-line statements like list comprehensions. They are called **drumroll** generator expressions!\n",
    "\n",
    "Here's an example of a list comprehension right on top of a generator expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = [i*10 for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = (i*10 for i in range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look basically the same, don't they? Apart from the parentheses/brackets, they almost are. The difference is that when you call ```lst``` it will return the same ten integers every time you call it. When you call ```next(gen)``` it will hit a hard stop once it's printed that tenth integer, and be inert thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators are a remarkable tool, largely unique to Python, and we data scientists can make great use of them. Their functionality extends well beyond parsing a single large file, and can be used to assess or analyze multiple large files. For example, if we wanted to find a specific word pattern across a thousand-plus text documents without tying up too many resources, we could use a generator to automate the search. It's even possible to build full pipelines using only generator functions!\n",
    "\n",
    "So, Dataquest community, what do you think? Will generators become a regular feature of your coding? Where else do you think they might be useful in data science?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
